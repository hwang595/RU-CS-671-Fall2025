<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 671: Recent Advances in Large Language Models</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .section-title {
            border-bottom: 2px solid #e5e7eb;
            padding-bottom: 0.5rem;
            margin-bottom: 1.5rem;
        }
        .paper-link {
            transition: color 0.2s ease-in-out;
            color: #2563eb;
            text-decoration: none;
        }
        .paper-link:hover {
            color: #1d4ed8;
            text-decoration: underline;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <!-- Header -->
    <header class="bg-white shadow-md">
        <div class="container mx-auto px-6 py-8">
            <h1 class="text-4xl font-bold text-gray-900">CS 16:198:671 - Recent Advances in Large Language Models</h1>
            <p class="text-xl text-gray-600 mt-2">Rutgers University, Department of Computer Science - Fall 2025</p>
            <div class="mt-4 flex flex-col sm:flex-row sm:items-center space-y-2 sm:space-y-0 sm:space-x-6 text-gray-700">
                <span><strong>Instructor:</strong> Dr. Hongyi Wang</span>
                <span><strong>Contact:</strong> hw689@cs.rutgers.edu</span>
                <span><strong>Office Hours:</strong> TBA</span>
            </div>
        </div>
    </header>

    <!-- Navigation -->
    <nav class="sticky top-0 bg-white/80 backdrop-blur-md z-10 shadow-sm">
        <div class="container mx-auto px-6 py-3">
            <ul class="flex justify-center space-x-6 sm:space-x-8">
                <li><a href="#overview" class="text-gray-600 hover:text-blue-500 font-medium transition">Overview</a></li>
                <li><a href="#schedule" class="text-gray-600 hover:text-blue-500 font-medium transition">Schedule</a></li>
                <li><a href="#readings" class="text-gray-600 hover:text-blue-500 font-medium transition">Reading List</a></li>
                <li><a href="#projects" class="text-gray-600 hover:text-blue-500 font-medium transition">Final Project</a></li>
            </ul>
        </div>
    </nav>

    <!-- Main Content -->
    <main class="container mx-auto px-6 py-12">

        <!-- Course Overview -->
        <section id="overview" class="mb-16 scroll-mt-24">
            <h2 class="text-3xl font-bold mb-6 section-title">Course Overview</h2>
            <div class="space-y-6 bg-white p-8 rounded-lg shadow-md">
                <div>
                    <h3 class="text-xl font-semibold mb-2">Description</h3>
                    <p class="text-gray-700 leading-relaxed">
                        This graduate-level course offers an in-depth exploration of the cutting-edge research and engineering principles behind large language models (LLMs). We will cover the foundational architectures that power modern AI, such as the Transformer, and delve into the complex systems required to train and serve these massive models at scale. Topics include advanced parallel computing paradigms (data, tensor, pipeline), specialized hardware like GPUs and accelerators, modern model architectures like Mixture-of-Experts (MoE), and the latest techniques in model optimization and alignment. A significant portion of the course is dedicated to student-led presentations and a final project, allowing students to engage directly with state-of-the-art literature and apply their knowledge to a practical problem.
                    </p>
                </div>
                <div>
                    <h3 class="text-xl font-semibold mb-2">Prerequisites</h3>
                    <p class="text-gray-700 leading-relaxed">
                        A strong background in machine learning, deep learning, and proficiency in Python programming are expected. Familiarity with deep learning frameworks like PyTorch or TensorFlow is highly recommended.
                    </p>
                </div>
            </div>
        </section>

        <!-- Schedule -->
        <section id="schedule" class="mb-16 scroll-mt-24">
            <h2 class="text-3xl font-bold mb-6 section-title">Schedule & Topics</h2>
            <div class="space-y-8">
                <!-- Part 1 -->
                <h3 class="text-2xl font-semibold text-gray-700">Part 1: Foundations & Parallelism (Instructor-Led)</h3>
                <div class="space-y-4">
                    <div class="bg-white p-6 rounded-lg shadow-sm border-l-4 border-blue-500">
                        <p class="font-bold text-lg">Week 1 (Sep 2-5): Introduction & Logistics</p>
                        <ul class="list-disc list-inside mt-2 text-gray-600">
                            <li>Course overview, grading, and project expectations.</li>
                            <li>The modern AI landscape.</li>
                        </ul>
                    </div>
                    <div class="bg-white p-6 rounded-lg shadow-sm border-l-4 border-blue-500">
                        <p class="font-bold text-lg">Week 2 (Sep 8-12): Foundation Model Architectures</p>
                        <p class="mt-2 text-gray-600">A deep dive into the Transformer architecture. Required readings: 
                            <a href="https://arxiv.org/abs/1706.03762" target="_blank" class="paper-link">Attention Is All You Need</a>, 
                            <a href="https://arxiv.org/abs/1810.04805" target="_blank" class="paper-link">BERT</a>, and
                            <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" class="paper-link">GPT-2</a>.
                        </p>
                    </div>
                    <div class="bg-white p-6 rounded-lg shadow-sm border-l-4 border-blue-500">
                        <p class="font-bold text-lg">Week 3 (Sep 15-19): Parallelisms I</p>
                        <p class="mt-2 text-gray-600">Topics on data, tensor, and pipeline parallelism. Readings: 
                            <a href="https://arxiv.org/abs/1909.08053" target="_blank" class="paper-link">Megatron-LM</a> and 
                            <a href="https://arxiv.org/abs/1811.06965" target="_blank" class="paper-link">GPipe</a>.
                        </p>
                    </div>
                     <div class="bg-white p-6 rounded-lg shadow-sm border-l-4 border-blue-500">
                        <p class="font-bold text-lg">Week 4 (Sep 22-26): Parallelisms II</p>
                        <p class="mt-2 text-gray-600">Advanced architectures (MOE, SSMs) and parallelisms. Readings: 
                            <a href="https://arxiv.org/abs/1910.02054" target="_blank" class="paper-link">ZeRO</a> and 
                            <a href="https://arxiv.org/abs/2201.05596" target="_blank" class="paper-link">ST-MOE</a>.
                        </p>
                    </div>
                     <div class="bg-white p-6 rounded-lg shadow-sm border-l-4 border-blue-500">
                        <p class="font-bold text-lg">Week 5 (Sep 29 - Oct 3): Hardware & Systems</p>
                        <p class="mt-2 text-gray-600">High-performance computing hardware (GPUs, accelerators), networking, and storage.</p>
                    </div>
                </div>

                <!-- Part 2 -->
                <h3 class="text-2xl font-semibold text-gray-700 mt-12">Part 2: Advanced Topics (Student-Led)</h3>
                 <div class="bg-white p-6 rounded-lg shadow-sm border-l-4 border-green-500">
                    <p class="font-bold text-lg">Weeks 6-14 (Oct 6 - Dec 5): Student-Led Paper Presentations</p>
                    <p class="mt-2 text-gray-600">Each week, students will lead discussions on selected papers from the course reading list. Note: Thanksgiving week (Nov 24-28) schedule will be adjusted.</p>
                </div>
                
                <!-- Part 3 -->
                <h3 class="text-2xl font-semibold text-gray-700 mt-12">Part 3: Final Projects</h3>
                 <div class="bg-white p-6 rounded-lg shadow-sm border-l-4 border-purple-500">
                    <p class="font-bold text-lg">Weeks 15-16 (Dec 8-19): Final Project Presentations</p>
                    <p class="mt-2 text-gray-600">Student teams will present their final course projects.</p>
                </div>
            </div>
        </section>

        <!-- Reading List -->
        <section id="readings" class="mb-16 scroll-mt-24">
            <h2 class="text-3xl font-bold mb-6 section-title">Reading List for Student Presentations</h2>
            <div class="bg-white p-8 rounded-lg shadow-md space-y-8">
                <div>
                    <h3 class="text-xl font-semibold text-gray-800 mb-3">Efficient Attention</h3>
                    <ul class="list-disc list-inside space-y-2 text-gray-700">
                        <li><a href="https://arxiv.org/abs/2205.14135" target="_blank" class="paper-link"><span class="font-semibold text-gray-800">FlashAttention:</span> Fast and Memory-Efficient Exact Attention with IO-Awareness</a> (Dao et al., 2022)</li>
                        <li><a href="https://arxiv.org/abs/2307.08691" target="_blank" class="paper-link"><span class="font-semibold text-gray-800">FlashAttention-2:</span> Faster Attention with Better Parallelism and Work Partitioning</a> (Dao, 2023)</li>
                    </ul>
                </div>
                <div>
                    <h3 class="text-xl font-semibold text-gray-800 mb-3">Efficient Inference & Serving</h3>
                    <ul class="list-disc list-inside space-y-2 text-gray-700">
                        <li><a href="https://arxiv.org/abs/2305.13245" target="_blank" class="paper-link"><span class="font-semibold text-gray-800">vLLM:</span> Efficient Memory Management for Large Language Model Serving with PagedAttention</a> (Kwon et al., 2023)</li>
                        <li><a href="https://arxiv.org/abs/2211.05102" target="_blank" class="paper-link"><span class="font-semibold text-gray-800">Speculative Decoding</span></a> (Leviathan et al., 2022)</li>
                    </ul>
                </div>
                 <div>
                    <h3 class="text-xl font-semibold text-gray-800 mb-3">Efficient Training & Fine-Tuning</h3>
                    <ul class="list-disc list-inside space-y-2 text-gray-700">
                        <li><a href="https://arxiv.org/abs/1710.03740" target="_blank" class="paper-link"><span class="font-semibold text-gray-800">Mixed Precision Training</span></a> (Micikevicius et al., 2017)</li>
                        <li><a href="https://arxiv.org/abs/2106.09685" target="_blank" class="paper-link"><span class="font-semibold text-gray-800">LoRA:</span> Low-Rank Adaptation of Large Language Models</a> (Hu et al., 2021)</li>
                    </ul>
                </div>
                 <div>
                    <h3 class="text-xl font-semibold text-gray-800 mb-3">Model Alignment</h3>
                    <ul class="list-disc list-inside space-y-2 text-gray-700">
                        <li><a href="https://arxiv.org/abs/2305.18290" target="_blank" class="paper-link"><span class="font-semibold text-gray-800">Direct Preference Optimization:</span> Your Language Model is Secretly a Reward Model</a> (Rafailov et al., 2023)</li>
                    </ul>
                </div>
                <div>
                    <h3 class="text-xl font-semibold text-gray-800 mb-3">Alternative Architectures</h3>
                    <ul class="list-disc list-inside space-y-2 text-gray-700">
                         <li><a href="https://arxiv.org/abs/2312.00752" target="_blank" class="paper-link"><span class="font-semibold text-gray-800">Mamba:</span> Linear-Time Sequence Modeling with Selective State Spaces</a> (Gu & Dao, 2023)</li>
                    </ul>
                </div>
                 <div>
                    <h3 class="text-xl font-semibold text-gray-800 mb-3">Scientific Applications</h3>
                    <ul class="list-disc list-inside space-y-2 text-gray-700">
                         <li><a href="https://www.nature.com/articles/s41586-021-03819-2" target="_blank" class="paper-link">Highly accurate protein structure prediction with <span class="font-semibold text-gray-800">AlphaFold</span></a> (Jumper et al., 2021)</li>
                    </ul>
                </div>
            </div>
        </section>
        
        <!-- Final Project -->
        <section id="projects" class="scroll-mt-24">
            <h2 class="text-3xl font-bold mb-6 section-title">Final Project</h2>
            <div class="bg-white p-8 rounded-lg shadow-md">
                 <p class="text-gray-700 leading-relaxed">
                    The final project is a significant component of this course. Students will work in teams to apply their knowledge to a practical problem in the field of large language models. This could involve reproducing results from a recent paper, exploring a novel model architecture, or developing a new application. Project presentations will take place during the final two weeks of the semester (Dec 8-19).
                </p>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="bg-gray-800 text-white mt-16">
        <div class="container mx-auto px-6 py-4 text-center">
            <p>&copy; 2025 Dr. Hongyi Wang | Department of Computer Science, Rutgers University</p>
        </div>
    </footer>

</body>
</html>

